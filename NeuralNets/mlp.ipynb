{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Implementation a Neural Network\n",
    "In this problem you should train an artificial neural network (MLP and RBF) for classification of the given dataset.<br>\n",
    "you should fill the missing sections to complete your neural network implementation.<br>\n",
    "in this exercise you will:\n",
    "- Tune the learning rate and regularization weight\n",
    "- Implement the loss function for MLP\n",
    "- Implement the forward pass and backward pass \n",
    "- visualize the train and validation accuracy versuse iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# initialization\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from neural_net import MLPNet\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1-1 Create a toy dataset\n",
    "In this section we create a toy dataset containing 5 samples with 5 features.\n",
    "The network has 6 neurons in the hidden layer and there are 3 classes in the output.\n",
    "MLPNet class in the file neural_net.py will be used to instantiate our network . We use toy dataset with small size to check your implementaion and to see what happens to network weights during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our network and toy dataset\n",
    "\n",
    "features = 5\n",
    "hidden_size = 6\n",
    "classes = 3\n",
    "inputs = 5\n",
    "np.random.seed(0)\n",
    "\n",
    "net = MLPNet(features, hidden_size, classes, std=1e-1)\n",
    "X = 10 * np.random.randn(inputs, features)\n",
    "y = np.array([1, 0, 0, 2, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1-2- Forward pass\n",
    "To run the following section, you should complete the loss function in the neural_net.py to compute scores . The output is a 5*3 matrix that contains the class score for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 6)\n",
      "(5, 6)\n",
      "(5, 6)\n",
      "(5, 6)\n",
      "(5, 6)\n",
      "(5, 6)\n",
      "[[ 0.01549474  0.03781625 -0.08877857]\n",
      " [-0.19807965 -0.03479121  0.0156349 ]\n",
      " [ 0.12302907  0.12023798 -0.03873268]\n",
      " [-0.03023028 -0.1048553  -0.14200179]\n",
      " [-0.17062702  0.19507754 -0.05096522]\n",
      " [-0.04380743 -0.12527954  0.07774904]]\n",
      "[[ 0.01549474  0.03781625 -0.08877857]\n",
      " [-0.19807965 -0.03479121  0.0156349 ]\n",
      " [ 0.12302907  0.12023798 -0.03873268]\n",
      " [-0.03023028 -0.1048553  -0.14200179]\n",
      " [-0.17062702  0.19507754 -0.05096522]\n",
      " [-0.04380743 -0.12527954  0.07774904]]\n",
      "[[ 0.01549474  0.03781625 -0.08877857]\n",
      " [-0.19807965 -0.03479121  0.0156349 ]\n",
      " [ 0.12302907  0.12023798 -0.03873268]\n",
      " [-0.03023028 -0.1048553  -0.14200179]\n",
      " [-0.17062702  0.19507754 -0.05096522]\n",
      " [-0.04380743 -0.12527954  0.07774904]]\n",
      "(5, 6)\n",
      "(5, 6)\n",
      "(5, 6)\n",
      "(5, 6)\n",
      "(5, 6)\n",
      "(5, 6)\n",
      "[[ 0.01549474  0.03781625 -0.08877857]\n",
      " [-0.19807965 -0.03479121  0.0156349 ]\n",
      " [ 0.12302907  0.12023798 -0.03873268]\n",
      " [-0.03023028 -0.1048553  -0.14200179]\n",
      " [-0.17062702  0.19507754 -0.05096522]\n",
      " [-0.04380743 -0.12527954  0.07774904]]\n",
      "[[ 0.01549474  0.03781625 -0.08877857]\n",
      " [-0.19807965 -0.03479121  0.0156349 ]\n",
      " [ 0.12302907  0.12023798 -0.03873268]\n",
      " [-0.03023028 -0.1048553  -0.14200179]\n",
      " [-0.17062702  0.19507754 -0.05096522]\n",
      " [-0.04380743 -0.12527954  0.07774904]]\n",
      "[[ 0.01549474  0.03781625 -0.08877857]\n",
      " [-0.19807965 -0.03479121  0.0156349 ]\n",
      " [ 0.12302907  0.12023798 -0.03873268]\n",
      " [-0.03023028 -0.1048553  -0.14200179]\n",
      " [-0.17062702  0.19507754 -0.05096522]\n",
      " [-0.04380743 -0.12527954  0.07774904]]\n",
      "(5, 6)\n",
      "(5, 6)\n",
      "(5, 6)\n",
      "(5, 6)\n",
      "(5, 6)\n",
      "(5, 6)\n",
      "[[ 0.01549474  0.03781625 -0.08877857]\n",
      " [-0.19807965 -0.03479121  0.0156349 ]\n",
      " [ 0.12302907  0.12023798 -0.03873268]\n",
      " [-0.03023028 -0.1048553  -0.14200179]\n",
      " [-0.17062702  0.19507754 -0.05096522]\n",
      " [-0.04380743 -0.12527954  0.07774904]]\n",
      "[[ 0.01549474  0.03781625 -0.08877857]\n",
      " [-0.19807965 -0.03479121  0.0156349 ]\n",
      " [ 0.12302907  0.12023798 -0.03873268]\n",
      " [-0.03023028 -0.1048553  -0.14200179]\n",
      " [-0.17062702  0.19507754 -0.05096522]\n",
      " [-0.04380743 -0.12527954  0.07774904]]\n",
      "[[ 0.01549474  0.03781625 -0.08877857]\n",
      " [-0.19807965 -0.03479121  0.0156349 ]\n",
      " [ 0.12302907  0.12023798 -0.03873268]\n",
      " [-0.03023028 -0.1048553  -0.14200179]\n",
      " [-0.17062702  0.19507754 -0.05096522]\n",
      " [-0.04380743 -0.12527954  0.07774904]]\n",
      "(5, 6)\n",
      "(5, 6)\n",
      "(5, 6)\n",
      "(5, 6)\n",
      "(5, 6)\n",
      "(5, 6)\n",
      "[[ 0.01549474  0.03781625 -0.08877857]\n",
      " [-0.19807965 -0.03479121  0.0156349 ]\n",
      " [ 0.12302907  0.12023798 -0.03873268]\n",
      " [-0.03023028 -0.1048553  -0.14200179]\n",
      " [-0.17062702  0.19507754 -0.05096522]\n",
      " [-0.04380743 -0.12527954  0.07774904]]\n",
      "[[ 0.01549474  0.03781625 -0.08877857]\n",
      " [-0.19807965 -0.03479121  0.0156349 ]\n",
      " [ 0.12302907  0.12023798 -0.03873268]\n",
      " [-0.03023028 -0.1048553  -0.14200179]\n",
      " [-0.17062702  0.19507754 -0.05096522]\n",
      " [-0.04380743 -0.12527954  0.07774904]]\n",
      "[[ 0.01549474  0.03781625 -0.08877857]\n",
      " [-0.19807965 -0.03479121  0.0156349 ]\n",
      " [ 0.12302907  0.12023798 -0.03873268]\n",
      " [-0.03023028 -0.1048553  -0.14200179]\n",
      " [-0.17062702  0.19507754 -0.05096522]\n",
      " [-0.04380743 -0.12527954  0.07774904]]\n",
      "(5, 6)\n",
      "(5, 6)\n",
      "(5, 6)\n",
      "(5, 6)\n",
      "(5, 6)\n",
      "(5, 6)\n",
      "[[ 0.01549474  0.03781625 -0.08877857]\n",
      " [-0.19807965 -0.03479121  0.0156349 ]\n",
      " [ 0.12302907  0.12023798 -0.03873268]\n",
      " [-0.03023028 -0.1048553  -0.14200179]\n",
      " [-0.17062702  0.19507754 -0.05096522]\n",
      " [-0.04380743 -0.12527954  0.07774904]]\n",
      "[[ 0.01549474  0.03781625 -0.08877857]\n",
      " [-0.19807965 -0.03479121  0.0156349 ]\n",
      " [ 0.12302907  0.12023798 -0.03873268]\n",
      " [-0.03023028 -0.1048553  -0.14200179]\n",
      " [-0.17062702  0.19507754 -0.05096522]\n",
      " [-0.04380743 -0.12527954  0.07774904]]\n",
      "[[ 0.01549474  0.03781625 -0.08877857]\n",
      " [-0.19807965 -0.03479121  0.0156349 ]\n",
      " [ 0.12302907  0.12023798 -0.03873268]\n",
      " [-0.03023028 -0.1048553  -0.14200179]\n",
      " [-0.17062702  0.19507754 -0.05096522]\n",
      " [-0.04380743 -0.12527954  0.07774904]]\n",
      "Your scores:\n",
      " [[array([-4.76888184, -0.31012331, -2.96622082, -3.65421661, -4.83108851,\n",
      "        0.41385497]), array([ 0.53925742, -1.13732454,  1.33071509])], [array([-1.07613449, -0.91277243, -1.11847698, -2.52745982, -1.04787432,\n",
      "        1.42001821]), array([ 0.21951616, -0.26072075,  0.64730221])], [array([-3.93358834e+00,  1.20895816e+00, -1.14667739e-03, -1.87757461e+00,\n",
      "       -3.79877740e+00, -6.97610940e-01]), array([ 0.43493318, -0.6477387 ,  0.77415062])], [array([-2.64241309, -0.04723502,  2.29682815, -5.08178134, -4.4985277 ,\n",
      "        3.91738197]), array([ 1.00057299, -0.65759463,  1.40035202])], [array([ 1.63023669, -2.04948118, -0.84690739, -1.89752301,  1.26031623,\n",
      "        2.39110826]), array([0.06459671, 0.17639074, 0.24715522])]]\n"
     ]
    }
   ],
   "source": [
    "scores = net.loss(X)\n",
    "print ('Your scores:\\n',scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1-3- Forward pass\n",
    "Now you should complete the second missing part in loss function in the neural_net.py , the output shows the loss of your network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss, _ = net.loss(X, y, reg=0.1)\n",
    "print(\"your network loss is:\",loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1-4- Backward pass\n",
    "compute the gradient according to weights and biases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss, grads = net.loss(X, y, reg=0.1)\n",
    "print('grads of W1\\n',grads['W1'])\n",
    "print('grads of b1\\n',grads['b1'])\n",
    "print('grads of W2\\n',grads['W2'])\n",
    "print('grads of b2\\n',grads['b2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1-1-5- Train the network\n",
    "Fill the missing section in train and predict function of MLPNet class . After you correctly implement those function you build a two layer neural network that can feed data to it. The result of the following section shows Final training loss of your network and training loss versus iteration ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = MLPNet(features, hidden_size, classes, std=1e-1)\n",
    "stats = net.train(X, y, X, y,\n",
    "            learning_rate=1e-1, reg=1e-5,\n",
    "            num_iters=100)\n",
    "\n",
    "print ('Final training loss: ', stats['loss_train'][-1])\n",
    "\n",
    "# plot the loss history\n",
    "plt.plot(stats['loss_train'])\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('training loss')\n",
    "plt.title('Training Loss history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1-6- Load data MNIST\n",
    "In this section we use a real dataset.change root variable in dataloader.py to your dataset path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dataloader import select_features\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imread\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "val_num = 1000\n",
    "train_num = 49000\n",
    "test_num = 10000\n",
    "train_data, train_labels, test_data, test_labels,\\\n",
    "    class_names, n_train, n_test, n_class, n_features = select_features()\n",
    "\n",
    "\n",
    "# Subsample the data\n",
    "mask = range(train_num, n_train)\n",
    "X_val = train_data[mask]\n",
    "y_val = train_labels[mask]\n",
    "mask = range(train_num)\n",
    "X_train = train_data[mask]\n",
    "y_train = train_labels[mask]\n",
    "mask = range(test_num)\n",
    "X_test = test_data[mask]\n",
    "y_test = test_labels[mask]\n",
    "\n",
    "# # Normalize the data: subtract the mean image\n",
    "# mean_image = np.mean(X_train, axis=0)\n",
    "# X_train -= mean_image\n",
    "# X_val -= mean_image\n",
    "# X_test -= mean_image\n",
    "\n",
    "print ('Train data shape: ', X_train.shape)\n",
    "print ('Train labels shape: ', y_train.shape)\n",
    "print ('Validation data shape: ', X_val.shape)\n",
    "print ('Validation labels shape: ', y_val.shape)\n",
    "print ('Test data shape: ', X_test.shape)\n",
    "print ('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1-7- Train \n",
    "For training our neural network we will use SGD(Stochastic Gradient descent).The loss of your network must decrease during epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size = n_features\n",
    "print(input_size)\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "net = MLPNet(input_size, hidden_size, num_classes,std=1e-2)\n",
    "\n",
    "# Train the network\n",
    "stats = net.train(X_train, y_train, X_val, y_val,\n",
    "            num_iters=30000, batch_size=200,\n",
    "            learning_rate=1e-4, learning_rate_decay=0.95,\n",
    "            reg=0.5)\n",
    "\n",
    "# Predict on the validation set\n",
    "val_acc = (net.predict(X_val) == y_val).mean()\n",
    "print ('Validation accuracy: ', val_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the below plots show loss during epochs and training and validatino accuracy versus epochs. The loss should decreas over time. ***according to second plot what is the best iteration number to terminate training?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the loss function and train / validation accuracies\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(stats['loss_train'])\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(stats['train_acc'], label='train')\n",
    "plt.plot(stats['val_acc'], label='val')\n",
    "plt.title('Classification accuracy ')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1-8- Tune network's hyperparameters\n",
    "As you know hidden layer size, learning rate and weight regularization are important in developing a neural network so you should tune them and get a better result and report the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_net = None # store the best model into this \n",
    "#################################################################################\n",
    "# Tune hyperparameters with validation set and store your best model in best_net#\n",
    "#################################################################################\n",
    "pass\n",
    "#################################################################################\n",
    "#                                END OF YOUR CODE                               #\n",
    "#################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1-9- Test\n",
    "Now you find the best hyperparameters, Let's test your network on test dataset and evaluate your accuracy.\n",
    "**our baseline is 47% and for each 1% you get 1 extra bonus point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_acc = (best_net.predict(X_test) == y_test).mean()\n",
    "print ('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2-1- Scikit-learn without normalization\n",
    "In this section you become familiar with Scikit-learn and implement an MLP network with that. Find the best parameters for best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2-2- Test acc\n",
    "test your network with test dataset and report accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2-3- Scikit-learn with normalization\n",
    "In this section we use previous code with just one change. befor you feed data to network normalized them . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2-4- Test accuracy\n",
    "Test again your network with test dataset.Do you see any progress in test accuracy against result of prevouis section?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
